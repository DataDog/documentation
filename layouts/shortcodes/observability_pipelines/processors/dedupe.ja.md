Deduplicate プロセッサーは、データのコピーを削除して、データ量とノイズを減らします。一度に 5,000 件のメッセージをキャッシュし、受信ログのトラフィックをキャッシュされているメッセージと比較します。たとえば、複数の同じ警告ログが連続して送信される場合、このプロセッサーを使用して、一意の警告ログだけを保持することができます。

Deduplicate プロセッサーをセットアップするには、次の手順に従います。
1. **フィルタークエリ**を定義します。指定された[フィルタークエリ](#filter-query-syntax)に一致するログのみが処理されます。重複ログとフィルタークエリに一致しないログは、パイプラインの次のステップに送信されます。
1. [**Type of deduplication**] ドロップダウンメニューで、以下のフィールドに対し `Match` と `Ignore` のいずれを行うかを選択します。
    - `Match` を選択した場合、ログが通過した後、今後作成されるログのうち、以下で指定するすべてのフィールドの値が同一であるログは削除されます。
    - `Ignore` を選択した場合、ログが通過した後、今後作成されるログのうち、以下で指定するフィールドを*除いた*すべてのフィールドの値が同一であるログが削除されます。
1. 一致させるフィールドまたは無視するフィールドを入力します。少なくとも 1 つのフィールドが必要です。最大 3 つのフィールドを指定できます。
    - サブフィールドに一致させるには、パス表記 `<OUTER_FIELD>.<INNER_FIELD>` を使用します。以下の[パス表記の例](#path-notation-example)を参照してください。
1. [**Add field**] をクリックして、フィルターを適用するフィールドを追加します。

##### パス表記の例

以下のメッセージ構造で値 `double_inner_value` を持つキーを参照するには、`outer_key.inner_key.double_inner_key` を使用します。
```json
{
    "outer_key": {
        "inner_key": "inner_value",
        "a": {
            "double_inner_key": "double_inner_value",
            "b": "b value"
        },
        "c": "c value"
    },
    "d": "d value"
}
```