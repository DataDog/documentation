datadog:
  apiKey: "REPLACE_ME"
  configKey: "REPLACE_ME"
  site: "datadoghq.com"

## Autoscaling
## 
autoscaling:
  enabled: true
  minReplicas: 2
  targetCPUUtilizationPercentage: 80

podDisruptionBudget:
  enabled: true
  minAvailable: 1

## HPA requires at least some resource requests to function, so we
## configure a reasonable default here. It is recommended that you
## change this to match the actual size of instances that you'll
## be using.
resources:
  requests:
    cpu: 1000m
    memory: 512Mi

affinity:
  ## It doesn't do much good to autoscale if a single DC can take
  ## your whole operation down, so we default to always running
  ## pods in different AZs.
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - observability-pipelines-worker
          topologyKey: topology.kubernetes.io/zone


## Load Balancing
## We've done our best to recommend a configuration that will
## avoid cross-AZ costs where possible.
service:
  enabled: true
  type: "LoadBalancer"
  externalTrafficPolicy: "Local"
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: "true"


## Buffering
## This creates an EBS drive that can be utilized for buffers, which
## must then be configured in the sinks themselves.
persistence:
  enabled: true
  storageClassName: "managed-csi"
  accessModes:
    - ReadWriteOnce
  size: 288Gi


## OP Configuration
## This is a sample configuration that we recommend to every user
## getting started with OP; it will get you proxying data as
## quickly as possible, as well as providing hooks for your own
## processing steps.
config:
  data_dir: /var/lib/observability-pipelines-worker

  sources:
    datadog_agent:
      address: 0.0.0.0:8282
      type: datadog_agent
      multiple_outputs: true

  transforms:
    ## The Datadog Agent natively encodes its tags as a comma-separated list
    ## values, stored in the string `.ddtags`. In order to actually work with
    ## and filter off of these tags, it's necessary to parse that string into
    ## more structured data.
    logs_parse_ddtags:
      type: remap
      inputs:
        - datadog_agent.logs
      source: |
        .ddtags = parse_key_value!(.ddtags, key_value_delimiter: ":", field_delimiter: ",")

    ## The `.status` attribute added by the Datadog Agent unfortunately does not
    ## convey any useful meaning, and if left alone will lead to your logs
    ## being mis-categorized when they are picked up by the intake.
    logs_remove_wrong_level:
      type: remap
      inputs:
        - logs_parse_ddtags
      source: |
        del(.status)

    ## This is a placeholder for your own remap (or other transform)
    ## steps, now with easier to deal-with tags. Keeping these tag assignments
    ## is recommended, however- they will help with classifying what
    ## traffic you have moved over to OP, and what still needs work.
    LOGS_YOUR_STEPS:
      type: remap
      inputs:
        - logs_remove_wrong_level
      source: |
        .ddtags.sender = "observability_pipelines_worker"
        .ddtags.opw_aggregator = get_hostname!()

    ## Before sending along to the Logs intake, we must re-encode the
    ## tags into the format that it expects, as though the agent was
    ## sending directly.
    logs_finish_ddtags:
      type: remap
      inputs:
        - LOGS_YOUR_STEPS
      source: |
        .ddtags = encode_key_value!(.ddtags, key_value_delimiter: ":", field_delimiter: ",")

    metrics_add_dd_tags:
      type: remap
      inputs:
        - datadog_agent.metrics
      source: |
        .tags.sender = "observability_pipelines_worker"
        .tags.opw_aggregator = get_hostname!()

  ## This buffer configuration is split into the following, totaling the 288GB
  ## provisioned above:
  ## - 240GB buffer for logs
  ## - 48GB buffer for metrics
  ##
  ## This will suffice for the vast majority of OPW deployments and should rarely
  ## need to be adjusted. If you do, be sure to adjust the size of the persistence:
  ## block above.
  ##
  ## The "${DD_API_KEY}" parameters are automatically filled in from the API
  ## key you filled in the datadog: block at the top of this file.
  sinks:
    datadog_logs:
      type: datadog_logs
      inputs:
        - logs_finish_ddtags
      default_api_key: "${DD_API_KEY}"
      compression: gzip
      buffer:
         type: disk
         max_size: 257698037760
    datadog_metrics:
      type: datadog_metrics
      inputs:
        - metrics_add_dd_tags
      default_api_key: "${DD_API_KEY}"
      buffer:
        type: disk
        max_size: 51539607552