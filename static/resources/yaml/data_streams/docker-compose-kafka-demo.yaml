# Kafka Consumer Integration Demo
# 
# Quick Start:
#   DD_API_KEY=your_key_here docker compose up
#
# Optional: Set DD_SITE (defaults to datadoghq.com)
#   DD_API_KEY=xxx DD_SITE=datadoghq.eu docker compose up
#
# This starts:
#   - Kafka (KRaft mode, no ZooKeeper) + Schema Registry
#   - Datadog Agent with Kafka Consumer integration
#   - Producer app (sends orders every 2s, APM + Data Streams enabled)
#   - Consumer app (processes orders, APM + Data Streams enabled)
#
# View in Datadog (wait 2-3 min):
#  - Navigate to Data Streams Monitoring and view your Kafka cluster & consumer / producer apps.

version: '3.8'

services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:19092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:19092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      CLUSTER_ID: 'Mka3OEVBNTcwNTJENDM2Qk'
    command: >
      bash -c "
      sed -i '/KAFKA_ZOOKEEPER_CONNECT/d' /etc/confluent/docker/configure &&
      /etc/confluent/docker/run
      "
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:19092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/"]
      interval: 10s
      timeout: 5s
      retries: 5

  datadog-agent:
    image: gcr.io/datadoghq/agent:latest
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    environment:
      DD_API_KEY: ${DD_API_KEY:?DD_API_KEY is required}
      DD_SITE: ${DD_SITE:-datadoghq.com}
      DD_HOSTNAME: kafka-demo-agent
      DD_TAGS: "env:demo kafka_cluster:dd-kafka-demo"
      DD_DATA_STREAMS_ENABLED: "true"
      DD_APM_ENABLED: "true"
      DD_APM_NON_LOCAL_TRAFFIC: "true"
      DD_DOGSTATSD_NON_LOCAL_TRAFFIC: "true"
      DD_LOG_LEVEL: "INFO"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc/:/host/proc/:ro
      - /sys/fs/cgroup/:/host/sys/fs/cgroup:ro
    configs:
      - source: kafka_consumer_config
        target: /etc/datadog-agent/conf.d/kafka_consumer.d/conf.yaml

  producer:
    image: python:3.11-slim
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      datadog-agent:
        condition: service_started
    environment:
      DD_SERVICE: order-producer
      DD_ENV: demo
      DD_VERSION: "1.0.0"
      DD_AGENT_HOST: datadog-agent
      DD_TRACE_AGENT_PORT: 8126
      DD_DATA_STREAMS_ENABLED: "true"
    configs:
      - source: order_proto
        target: /app/order.proto
      - source: producer_script
        target: /app/producer.py
    command: >
      bash -c "
      apt-get update -qq &&
      apt-get install -y -qq gcc g++ librdkafka-dev protobuf-compiler &&
      pip install -q protobuf==3.20.3 confluent-kafka[protobuf]==2.3.0 ddtrace>=2.0.0 &&
      cd /app &&
      protoc --python_out=. order.proto &&
      ddtrace-run python producer.py
      "
    restart: unless-stopped

  consumer:
    image: python:3.11-slim
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      datadog-agent:
        condition: service_started
    environment:
      DD_SERVICE: order-consumer
      DD_ENV: demo
      DD_VERSION: "1.0.0"
      DD_AGENT_HOST: datadog-agent
      DD_TRACE_AGENT_PORT: 8126
      DD_DATA_STREAMS_ENABLED: "true"
    configs:
      - source: order_proto
        target: /app/order.proto
      - source: consumer_script
        target: /app/consumer.py
    command: >
      bash -c "
      apt-get update -qq &&
      apt-get install -y -qq gcc g++ librdkafka-dev protobuf-compiler &&
      pip install -q protobuf==3.20.3 confluent-kafka[protobuf]==2.3.0 ddtrace>=2.0.0 &&
      cd /app &&
      protoc --python_out=. order.proto &&
      ddtrace-run python consumer.py
      "
    restart: unless-stopped

configs:
  kafka_consumer_config:
    content: |
      init_config:

      instances:
        - kafka_connect_str:
            - kafka:19092
          enable_cluster_monitoring: true
          schema_registry_url: http://schema-registry:8081
          tags:
            - env:demo
            - cluster:kafka-demo

  order_proto:
    content: |
      syntax = "proto3";
      package ecommerce;
      message Order {
        string order_id = 1;
        string customer_id = 2;
        string product_name = 3;
        int32 quantity = 4;
        double price = 5;
        double total_amount = 6;
        string region = 7;
        int64 timestamp = 8;
      }

  producer_script:
    content: |
      #!/usr/bin/env python3
      import os, time, random
      from confluent_kafka import Producer
      from confluent_kafka.serialization import SerializationContext, MessageField
      from confluent_kafka.schema_registry import SchemaRegistryClient
      from confluent_kafka.schema_registry.protobuf import ProtobufSerializer
      from ddtrace import tracer, patch
      patch(kafka=True)
      from order_pb2 import Order

      PRODUCTS = [('laptop', 999.99), ('mouse', 29.99), ('keyboard', 79.99), ('monitor', 299.99)]
      CUSTOMERS = ['alice', 'bob', 'charlie', 'diana']
      REGIONS = ['us-east', 'us-west', 'eu-central']

      def delivery_report(err, msg):
          if err: print(f'âŒ Delivery failed: $${err}')
          else: print(f'âœ… Order delivered to $${msg.topic()} [$${msg.partition()}] offset $${msg.offset()}')

      schema_client = SchemaRegistryClient({'url': 'http://schema-registry:8081'})
      serializer = ProtobufSerializer(Order, schema_client, {'use.deprecated.format': False})
      producer = Producer({'bootstrap.servers': 'kafka:19092', 'client.id': 'order-producer'})

      print("ðŸš€ Producer started - sending orders every 2 seconds...")
      count = 0
      try:
          while True:
              with tracer.trace("produce.order", service="order-producer"):
                  product, price = random.choice(PRODUCTS)
                  qty = random.randint(1, 5)
                  order = Order(
                      order_id=f"ORD-$${int(time.time()*1000)}",
                      customer_id=random.choice(CUSTOMERS),
                      product_name=product, quantity=qty, price=price,
                      total_amount=price*qty, region=random.choice(REGIONS),
                      timestamp=int(time.time())
                  )
                  count += 1
                  print(f"ðŸ“¦ Order #$${count}: $${order.order_id} | $${order.customer_id} | $${product} x$${qty} | $$$${order.total_amount:.2f}")
                  ctx = SerializationContext('demo-orders', MessageField.VALUE)
                  producer.produce('demo-orders', key=order.customer_id.encode('utf-8'),
                                 value=serializer(order, ctx), on_delivery=delivery_report)
                  producer.poll(0)
              time.sleep(2)
      except KeyboardInterrupt:
          print("\nðŸ›‘ Shutting down...")
      finally:
          producer.flush()

  consumer_script:
    content: |
      #!/usr/bin/env python3
      import os, time
      from confluent_kafka import Consumer, KafkaError
      from confluent_kafka.serialization import SerializationContext, MessageField
      from confluent_kafka.schema_registry import SchemaRegistryClient
      from confluent_kafka.schema_registry.protobuf import ProtobufDeserializer
      from ddtrace import tracer, patch
      patch(kafka=True)
      from order_pb2 import Order

      schema_client = SchemaRegistryClient({'url': 'http://schema-registry:8081'})
      deserializer = ProtobufDeserializer(Order, {'use.deprecated.format': False})
      consumer = Consumer({
          'bootstrap.servers': 'kafka:19092',
          'group.id': 'demo-order-processor',
          'client.id': 'order-consumer',
          'auto.offset.reset': 'earliest'
      })
      consumer.subscribe(['demo-orders'])

      print("ðŸš€ Consumer started - processing orders...")
      count = 0
      try:
          while True:
              msg = consumer.poll(1.0)
              if msg is None: continue
              if msg.error():
                  if msg.error().code() != KafkaError._PARTITION_EOF:
                      print(f"âŒ Error: $${msg.error()}")
                  continue
              
              with tracer.trace("consume.order", service="order-consumer"):
                  count += 1
                  ctx = SerializationContext('demo-orders', MessageField.VALUE)
                  order = deserializer(msg.value(), ctx)
                  print(f"ðŸ“¬ Message #$${count} [P$${msg.partition()}:$${msg.offset()}]")
                  
                  with tracer.trace("process.order"):
                      time.sleep(0.1)
                      if order.total_amount > 500: time.sleep(0.3)
                      print(f"âœ… Processed $${order.order_id} | $${order.customer_id} | $$$${order.total_amount:.2f}")
      except KeyboardInterrupt:
          print("\nðŸ›‘ Shutting down...")
      finally:
          consumer.close()

networks:
  default:
    name: kafka-demo
