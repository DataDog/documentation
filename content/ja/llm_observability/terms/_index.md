---
aliases:
- /ja/tracing/llm_observability/core_concepts
- /ja/llm_observability/core_concepts
- /ja/tracing/llm_observability/span_kinds
- /ja/llm_observability/span_kinds
further_reading:
- link: /llm_observability/setup
  tag: ドキュメント
  text: LLM Observability のセットアップ方法
- link: /llm_observability/submit_evaluations
  tag: ガイド
  text: LLM Observability への評価の送信
title: LLM Observability の用語と概念
---

{{< site-region region="gov" >}}
<div class="alert alert-warning">選択したサイト ({{< region-param key="dd_site_name" >}}) では現在 LLM Observability は利用できません。</div>
{{< /site-region >}}

## 概要

LLM Observability UI には、会話のパフォーマンスを診断し、製品全体でデータを相関させるための多くのツールが提供されています。これにより、大規模言語モデル (LLM) の問題を特定し、解決することができます。

| コンセプト | 説明 |
|---|---|
| [スパン](#spans) | スパンとは、LLM アプリケーション内で行われる操作を表す作業単位であり、トレースの構成要素です。 |
| [トレース](#traces) | トレースは、LLM アプリケーションでリクエストを処理する際の作業を示し、1 つ以上のネストされたスパンで構成されます。ルートスパンはトレースの最初のスパンであり、トレースの開始と終了を表します。 |
| [評価](#evaluations) | 評価とは、LLM アプリケーションのパフォーマンスを測定する手法です。例えば、応答がない場合やトピックの関連性などの品質チェックは、LLM アプリケーションで追跡できる評価の種類です。 |

## スパン

スパンは、以下の属性で構成されます。

- 名前
- 開始時間と継続時間
- エラーの種類、メッセージ、トレースバック
- LLM のプロンプトや補完などの入力と出力
- メタデータ (例えば、`temperature`、`max_tokens` などの LLM パラメーター)
- メトリクス (`input_tokens` や `output_tokens` など)
- タグ

### スパンの種類

LLM Observability は、スパンが実行している作業の種類を定義する*スパン種別*によってスパンを分類します。これにより、LLM アプリケーションで実行されている操作について、より詳細な洞察を得ることができます。

LLM Observability は、次のスパン種別をサポートしています。

| 種別      | 表すもの   | ルートスパンかどうか   | 子スパンを持つことができるかどうか | 例 |
|-----------|--------------|--------------|-------------|----|
| [LLM](#llm-span)      | LLM への呼び出し。 | はい | いいえ | OpenAI GPT-4 などのモデルへの呼び出し。 |
| [ワークフロー](#workflow-span)  | LLM の呼び出しやその周辺のコンテキスト操作を含む、あらかじめ決められた操作のシーケンス。 | はい | はい | URL を受け取り、ページの要約を返すサービス。これには、ページ取得のためのツール呼び出し、テキスト処理タスク、LLM による要約が含まれます。 |
| [Agent](#agent-span)     | 自律エージェントによる一連の決定と操作。通常、ネストされたワークフロー、LLM、ツール、タスク呼び出しで構成されます。 | はい | はい  | 顧客の質問に答えるチャットボット。
| [ツール](#tool-span)      | LLM が生成した引数で外部プログラムやサービスに呼び出しを行うこと。 | いいえ | いいえ | Web 検索 API や電卓への呼び出し。 |
| [タスク](#task-span)      | 外部サービスを呼び出さないスタンドアロンのステップ。 | いいえ | いいえ | データの前処理ステップ。 |
| [埋め込み](#embedding-span) | 埋め込みを返すモデルや関数への呼び出し。 | いいえ  | はい | text-embedding-ada-002 への呼び出し。 |
| [取得](#retrieval-span) | 外部の知識ベースからデータを取得する操作。 | いいえ | いいえ | ベクトルデータベースへの呼び出しで、ランク付けされた文書の配列を返します。 |

アプリケーションからスパンを作成する手順については、コード例を含む詳細が記載された LLM Observability SDK for Python のドキュメント内の[スパンのトレース][2]を参照してください。

#### LLM スパン

LLM スパンは、LLM への呼び出しを表し、入力と出力はテキストで表されます。

トレースには LLM スパンが 1 つ含まれることがあり、その場合は LLM の推論操作を表します。

LLM スパンは通常、子スパンを持たず、LLM への直接呼び出しを表すスタンドアロンの操作です。

#### ワークフロースパン

ワークフロースパンは、*静的な*操作のシーケンスを表します。ワークフローを使用して、LLM 呼び出しとツール呼び出し、データ取得、その他のタスクなどのサポート的なコンテキスト操作をグループ化します。

ワークフロースパンは、標準的な操作のシーケンスで構成されるトレースのルートスパンとなることが多いです。例えば、arXiv 論文のリンクを受け取り、その要約を返すプロセスです。このプロセスには、ツール呼び出し、テキスト処理タスク、LLM による要約が含まれます。

ワークフロースパンは、ワークフローシーケンス内の子ステップを表す子スパンを持つことができます。

#### Agent スパン

エージェントスパンは、入力に基づいて大規模言語モデルが操作を決定し実行する動的な操作シーケンスを表します。例えば、エージェントスパンは [ReAct エージェント][1]によって制御される推論ステップの連続を表すことがあります。

エージェントスパンは、自律エージェントや推論エージェントを表すトレースのルートスパンとなることがよくあります。

エージェントスパンは、推論エンジンによって制御される子ステップを表す任意のスパンを子として持つことができます。

#### ツールスパン

ツールスパンは、ワークフローやエージェントにおいて、Web API やデータベースなどの外部プログラムやサービスへの呼び出しを含むスタンドアロンステップを表します。

ツールスパンは通常、子スパンを持たず、ツールの実行を表すスタンドアロン操作です。

#### タスクスパン

タスクスパンは、ワークフローやエージェントにおいて、外部サービスへの呼び出しを伴わないスタンドアロンステップを表します。例えば、プロンプトを LLM に送信する前のデータ無害化ステップなどです。

タスクスパンは通常、子スパンを持たず、ワークフローやエージェント内のスタンドアロンステップです。

#### 埋め込みスパン

埋め込みスパンはツールスパンのサブカテゴリーであり、埋め込みを作成するための埋め込みモデルや関数へのスタンドアロン呼び出しを表します。例えば、埋め込みスパンは OpenAI の埋め込みエンドポイントへの呼び出しをトレースするために使用されます。

埋め込みスパンはタスクスパンを子として持つことができますが、通常は子を持ちません。

#### 取得スパン

取得スパンは、ツールスパンのサブカテゴリーであり、外部の知識ベースから返される文書のリストを含むベクトル検索操作を表します。例えば、取得スパンは、特定のトピックに関するユーザープロンプトを拡張するために関連する文書を収集するベクトルストアへの類似性検索をトレースするために使用されます。

埋め込みスパンと併用することで、取得スパンは、取得拡張生成 (RAG) 操作の可視化を提供することができます。

取得スパンは通常、子スパンを持たず、スタンドアロンの取得ステップを表します。

## トレース

LLM Observability は、さまざまな複雑性を持つ LLM アプリケーションに対する可観測性をサポートします。トレースの構造と複雑性に基づいて、LLM Observability の以下の機能を使用できます。

### LLM 推論のモニタリング

LLM 推論トレースは単一の LLM スパンで構成されます。

{{< img src="llm_observability/llm-observability-llm-span.png" alt="単一の LLM スパン" style="width:100%;" >}}

個々の LLM 推論をトレースすると、LLM Observability の基本機能が有効になり、以下のことが可能になります。

1. LLM 呼び出しへの入力と出力を追跡します。
2. LLM 呼び出しのトークン使用状況、エラーレート、レイテンシーを追跡します。
3. モデルおよびモデルプロバイダー別に重要なメトリクスを分解します。


詳細な例については、LLM 呼び出しの作成とトレース方法を示す [LLM モニタリング Jupyter ノートブック][7]をご覧ください。

SDK は、特定のプロバイダーに対する LLM 呼び出しを自動的にキャプチャするインテグレーションを提供します。詳細は、[自動インスツルメンテーション][3]を参照してください。サポートされていない LLM プロバイダーを使用している場合は、[手動でアプリケーションにインスツルメンテーションを適用][4]する必要があります。

### LLM ワークフローモニタリング

ワークフロートレースは、ネストされた LLM、タスク、ツール、埋め込み、および取得のスパンを持つルートワークフロースパンで構成されます。

{{< img src="llm_observability/llm-observability-workflow-trace.png" alt="より複雑な LLM ワークフローを可視化するトレース" style="width:100%;" >}}

ほとんどの LLM アプリケーションには、LLM 呼び出しを囲む操作が含まれており、例えば外部 API へのツール呼び出しや前処理タスクステップなど、アプリケーション全体のパフォーマンスに大きな役割を果たします。

LLM 呼び出しとコンテキストのあるタスクまたはツール操作をワークフロースパンで一緒にトレースすることで、LLM アプリケーションのより詳細な洞察とより全体的なビューを得ることができます。

詳細な例については、ツール呼び出しと LLM への呼び出しを含む複雑で静的な一連のステップを作成し、トレースする方法を示す [LLM モニタリング Jupyter ノートブック][8]、または RAG ワークフローの作成、トレース、評価の方法を説明する [LLM モニタリング Jupyter ノートブック][10]をご覧ください。

### LLM Agent モニタリング

エージェントモニタリングトレースは、ネストされた LLM、タスク、ツール、埋め込み、取得、およびワークフロースパンを持つルートエージェントスパンで構成されています。

{{< img src="llm_observability/llm-observability-agent-trace.png" alt="LLM エージェントを可視化するトレース" style="width:100%;" >}}

LLM アプリケーションに、静的なワークフローではキャプチャできない意思決定などの複雑な自律ロジックがある場合、LLM エージェントを使用している可能性が高いです。エージェントは、ユーザー入力に応じて複数の異なるワークフローを実行することがあります。

LLM アプリケーションをインスツルメンテーションして、単一の LLM エージェントによって実行されたすべてのワークフローとコンテキスト操作をエージェントトレースとしてまとめてトレースすることができます。

詳細な例については、ツールを呼び出し、データに基づいて意思決定を行う LLM 搭載のエージェントを作成し、トレースする方法を示す [LLM モニタリング Jupyter ノートブック][9]をご覧ください。

## 評価

LLM Observability は、LLM 会話の品質と有効性を評価するための品質チェックとすぐに使えるメトリクスを提供します。これには、感情、トピックの関連性、ユーザー満足度の判定が含まれます。評価により、会話のパフォーマンスを把握し、LLM アプリケーションの応答を強化することができます。これにより、ユーザーエクスペリエンスが向上し、価値のある正確な出力が保証されます。

{{< img src="llm_observability/evaluations/evaluations.png" alt="LLM Observability における品質評価" style="width:100%;" >}}

会話の評価に加えて、LLM Observability は [Sensitive Data Scanner][5] とインテグレーションし、会話中に存在する可能性がある機密情報 (個人情報、財務情報、または企業秘密など) を識別し、フラグを立てることで、データ漏洩を防止します。

LLM Observability は、機密データを積極的にスキャンすることで、会話が安全に保たれ、データ保護規制に準拠していることを保証します。この追加のセキュリティレイヤーにより、Datadog は LLM とのユーザーインタラクションの機密性とインテグレーションを維持するというコミットメントを強化します。

LLM Observability は、評価を個々のスパンに関連付けるため、特定の評価につながった入力と出力を表示できます。Datadog はトレースにすぐに使える評価を提供していますが、LLM Observability に[独自の評価を送信][6]することもできます。

### 品質評価

#### トピックの関連性

このチェックでは、構成された許容入力トピックから逸脱したユーザー入力を識別し、フラグを付けます。これにより、LLM の指定した目的と範囲に該当するやりとりが維持されます。

{{< img src="llm_observability/evaluations/topic_relevancy_1.png" alt="LLM Observability における LLM によって検出されたトピック関連性評価" style="width:100%;" >}}

| 評価段階 | 評価方法 | 評価定義 | 
|---|---|---|
| 入力に基づいて評価 | LLM を使用して評価 | トピックの関連性は、各プロンプトと応答のペアが、大規模言語モデル (LLM) アプリケーションの意図する主題に適合しているかどうかを判定します。例えば、e コマースのチャットボットがピザのレシピに関する質問を受けた場合、関連性がないとフラグ付けされます。  |

#### 回答の失敗

このチェックでは、LLM が適切な応答を提供できなかった場合を特定します。これは、LLM の知識や理解の限界、ユーザーの問い合わせの曖昧さ、またはトピックの複雑さが原因で発生することがあります。

{{< img src="llm_observability/evaluations/failure_to_answer_1.png" alt="LLM Observability における LLM で検出された回答失敗評価" style="width:100%;" >}}

| 評価段階 | 評価方法 | 評価定義 | 
|---|---|---|
| 出力に基づいて評価 | LLM を使用して評価 | 回答の失敗は、各プロンプトと応答のペアが、LLM アプリケーションがユーザーの質問に対して適切で満足のいく回答を提供しているかどうかをフラグ付けします。  |

#### 言語の不一致

このチェックでは、LLM がユーザーが使用している言語や方言とは異なる言語で応答を生成する場合を特定します。これにより、混乱や誤解が生じる可能性があります。このチェックにより、LLM の応答がユーザーの言語的な好みやニーズに合致していることを確認します。

{{< img src="llm_observability/evaluations/language_mismatch_1.png" alt="LLM Observability におけるオープンソースモデルで検出された言語の不一致評価" style="width:100%;" >}}

| 評価段階 | 評価方法 | 評価定義 | 
|---|---|---|
| 入力と出力に基づいて評価 | オープンソースモデルを使用して評価 | 言語の不一致評価は、各プロンプトと応答のペアが、LLM アプリケーションがユーザーの質問に対してユーザーが使用したのと同じ言語で回答しているかどうかを示します。  |

#### 感情

このチェックは、会話の全体的な雰囲気を理解し、ユーザーの満足度を測定し、感情の傾向を特定し、感情的な反応を解釈するのに役立ちます。このチェックは、テキストの感情を正確に分類し、ユーザー体験を改善し、ユーザーのニーズにより良く応えるための応答を調整する洞察を提供します。

{{< img src="llm_observability/evaluations/sentiment_1.png" alt="LLM Observability における LLM によって検出された感情評価" style="width:100%;" >}}

| 評価段階 | 評価方法 | 評価定義 | 
|---|---|---|
| 入力と出力に基づいて評価 | LLM を使用して評価 | 感情の評価は、テキストの感情的なトーンや態度をフラグ付けし、ポジティブ、ネガティブ、またはニュートラルに分類します。   |

### セキュリティおよび安全性の評価

#### 有害性

このチェックでは、ユーザーの入力プロンプトおよび LLM アプリケーションからの応答に有害なコンテンツが含まれているかを評価します。このチェックにより、有害な内容を識別し、フラグを立て、やりとりが常に安全で敬意を持って行われることを保証します。

{{< img src="llm_observability/evaluations/toxicity_1.png" alt="LLM Observability における LLM によって検出された有害性評価" style="width:100%;" >}}

| 評価段階 | 評価方法 | 評価定義 | 
|---|---|---|
| 入力と出力に基づいて評価 | LLM を使用して評価 | 有害性の評価は、ヘイトスピーチ、嫌がらせ、脅迫など、あらゆる有害、攻撃的、または不適切な言語や行動をフラグ付けします。 |

#### プロンプト注入

このチェックでは、認可されていない、または悪意のある作成者が、LLM の応答を操作したり、元の作成者が意図しない方向に会話を差し向けようとする試みを特定します。このチェックにより、ユーザーと LLM 間のやり取りの整合性と信頼性が維持されます。

{{< img src="llm_observability/evaluations/prompt_injection_1.png" alt="LLM Observability における LLM によって検出されたプロンプト注入評価" style="width:100%;" >}}

| 評価段階 | 評価方法 | 評価定義 | 
|---|---|---|
| 入力に基づいて評価 | LLM を使用して評価 | プロンプト注入評価は、外部の第三者やユーザーによって、認可されていないまたは悪意のあるプロンプトや指示が会話に注入された場合にフラグを立てます。 |

#### 機密データスキャン

このチェックにより、機密情報が適切かつ安全に処理されていることを確認し、データ漏洩や不正アクセスのリスクを低減します。

{{< img src="llm_observability/evaluations/sensitive_data_scanning_1.png" alt="LLM Observability における Sensitive Data Scanner によって検出されたセキュリティおよび安全性評価" style="width:100%;" >}}

| 評価段階 | 評価方法 | 評価定義 | 
|---|---|---|
| 入力と出力に基づいて評価 | Sensitive Data Scanner | [Sensitive Data Scanner][5] の機能により、LLM Observability は、LLM アプリケーションのすべてのプロンプトと応答のペアをスキャンし、機密情報を特定してマスキングします。これには、個人情報、財務データ、医療記録、その他プライバシーやセキュリティ上の理由で保護が必要なデータが含まれます。 |

## その他の参考資料

{{< partial name="whats-next/whats-next.html" >}}

[1]: https://react-lm.github.io/
[2]: /ja/llm_observability/setup/sdk/?tab=model#tracing-spans
[3]: /ja/llm_observability/setup/auto_instrumentation/
[4]: /ja/llm_observability/setup/?tab=decorators#instrument-your-llm-application
[5]: /ja/sensitive_data_scanner/
[6]: /ja/llm_observability/submit_evaluations
[7]: https://github.com/DataDog/llm-observability/blob/main/1-llm-span.ipynb
[8]: https://github.com/DataDog/llm-observability/blob/main/2-workflow-span.ipynb
[9]: https://github.com/DataDog/llm-observability/blob/main/3-agent-span.ipynb
[10]: https://github.com/DataDog/llm-observability/blob/main/4-custom-evaluations.ipynb