---
aliases:
- /ko/observability_pipelines/architecture/advanced_configurations/
title: (레거시) 고급 설정
---

<div class="alert alert-info">
본 지침은 대규모 프로덕션 수준 배포용입니다.
</div>

### 다중 애그리게이터(aggregator) 배포

[네트워킹][1] 항목에서 다룬 바와 같이, Datadog은 리전당 하나의 관측성 파이프라인 작업자 애그리게이터(aggregator)로 시작할 것을 권장합니다. 이는 관측성 파이프라인 작업자의 초기 배포가 지나치게 복잡해지는 것을 방지하기 위한 것이지만, 다음과 같이 다중 배포로 시작하는 것이 적합한 상황도 있습니다.

1. **공용 인터넷을 통한 데이터 전송 방지.** 다중 클라우드와 리전이 있는 경우, 각 클라우드와 리전에 관측성 파이프라인 작업자 애그리게이터(aggregator)를 배포하여 인터넷을 통해 다량의 데이터가 전송되는 것을 방지합니다. 관측성 파이프라인 작업자 애그리게이터(aggregator)는 내부 데이터를 수신하고 네트워크의 단일 출구 포인트 역할을 해야 합니다.

2. **독립적 관리.** 사용 사례에 따라 관측성 파이프라인 작업자 애그리게이터(aggregator)를 독립적으로 운영 및 관리할 수 있는 팀이 있습니다. 예를 들어, 데이터 사이언스 팀은 자체적 인프라스트럭처 운영 책임을 지고 자체 가시성 파이프라인 작업자 애그리게이터(aggregator)를 독립적으로 운영할 수단을 가지고 있을 수 있습니다.

### 다중 클라우드 계정

많은 사용자가 VPC와 클러스터 내부에 다중 클라우드 계정을 가지고 있습니다. Datadog은 이러한 경우에도 여전히 리전당 하나의 관측성 파이프라인 작업자 애그리게이터(aggregator)를 배포할 것을 권장합니다. 유틸리티 또는 툴 클러스터에 관측성 파이프라인 작업자를 배포하여 모든 클라우드 계정이 해당 클러스터로 데이터를 전송하도록 설정하세요. 자세한 내용은 [네트워킹][1]을 참조하세요.

### Pub-Sub 시스템

아키텍처를 고가용성 또는 고내구성으로 만들기 위해서 Kafka 같은 Pub-Sub 시스템을 사용할 필요는 없지만([고가용성 및 장애 복구][2] 항목 참조), 다음과 같은 이점이 있습니다.

1. **안정성이 향상됩니다.** Pub-Sub 시스템은 자주 변경되지 않는 매우 안정적이고 내구성 있는 시스템으로 설계되었습니다. 관리형 옵션을 사용하는 경우 특히 더 안정적입니다. 관측성 파이프라인 작업자는 목적에 따라 더 자주 변경될 가능성이 높습니다. 클라이언트 인식의 가용성을 높이고 복구 절차를 보다 간단하게 만들려면 관측성 파이프라인 작업자 다운타임을 Pub-Sub 시스템 뒤로 따로 분리합니다.


2. **로드 밸런서가 필요하지 않습니다.** Pub-Sub 시스템에서 로드 밸런서를 사용할 필요가 없습니다. Pub-Sub 시스템이 컨슈머 조정을 처리하므로 관측성 파이프라인 작업자를 더 쉽게 수평 확장할 수 있습니다.

#### Pub-Sub 파티셔닝

파티셔닝, 즉 Kafka 용어로 '토픽'은 Pub-Sub 시스템에서 데이터를 분리하는 것을 의미합니다. 데이터를 생성한 서비스 또는 호스트 등 데이터 출처 라인을 따라 파티셔닝합니다.

{{< img src="observability_pipelines/production_deployment_overview/partitioning.png" alt="노드의 에이전트가  Pub-Sub로 4개 서비스에 데이터를 전송한 후, 해당 데이터를 4개 관측성 파이프라인 작업자에 전송하는 것을 보여주는 다이어그램." style="width:55%;" >}}

#### Pub-Sub 설정

Pub-Sub 시스템을 사용하는 경우, Datadog은 관측성 파이프라인 작업자에 대해 다음과 같은 설정 변경을 권장합니다.

- **모든 싱크에 대해 엔드 투 엔드 승인 사용.** 본 설정은 데이터가 성공적으로 기록될 때까지 Pub-Sub 체크포인트가 진행되지 않도록 합니다.
- **메모리 버퍼를 사용합니다.** 관측성 파이프라인 작업자의 디스크 버퍼가 Pub-Sub 시스템 배경에 있을 때는 이를 사용할 필요가 없습니다. Pub-Sub 시스템은 높은 내구성을 갖춘 장기 버퍼링을 위해 설계되었습니다. 관측성 파이프라인 작업자는 데이터 읽기, 처리, 라우팅(내구성이 아닌)만 담당해야 합니다.

### 전역 집계

본 섹션에서는 레거시 대상에 대한 전역 계산을 수행하기 위한 권장 사항을 제공해 드립니다. 최신 대상은 이미 전역 계산을 지원합니다. 예를 들어 Datadog은 메트릭 데이터의 전역 관측성을 해결하는 분포(예: DDSketch)를 지원합니다.

전역 집계는 전체 리전에 대한 데이터를 집계하는 기능을 뜻합니다. 예를 들어, CPU 로드 평균에 대한 전역 사분위수를 계산할 수 있습니다. 이를 계산하려면 단일 관측성 파이프라인 작업자 인스턴스가 모든 노드의 CPU 로드 평균 통계에 액세스할 수 있어야 합니다. 수평 확장 시에는 불가능하며, 각 개별 관측성 파이프라인 작업자 인스턴스는 전체 데이터의 일부만 액세스할 수 있습니다. 따라서 집계는 계층화되어야 합니다.

{{< img src="observability_pipelines/production_deployment_overview/global_aggregation.png" alt="티어 1 애그리게이터(aggregator)에 데이터를 전송하는 로드 밸런서를 표시하는 다이어그램. 해당 애그리게이터(aggregator)에는 다중 관측성 파이프라인 작업자가 있습니다. 그런 다음 티어 1 데이터가 하나의 작업자를 갖는 티어 2 애그리게이터(aggregator)에 전송됩니다." style="width:90%;" >}}

위 다이어그램에서 티어 2 애그리게이터는 티어 1 애그리게이터로부터 전체 데이터의 집계된 하위 스트림을 받습니다. 이러한 방법으로 단일 인스턴스가 전체 스트림을 처리하지 않고 단일 장애점을 발생시키지 않고도 전역 보기를 얻을 수 있습니다.

#### 권장 사항

- 전역 집계는 전역 히스토그램 컴퓨팅과 같이 데이터를 줄일 수 있는 작업으로 제한합니다. 모든 데이터를 전역 애그리게이터로 전송하지 않습니다.
- 로컬 애그리게이터를 계속 사용해 대부분의 데이터를 처리 및 전송하여 단일 장애점이 발생하지 않도록 합니다.

[1]: /ko/observability_pipelines/legacy/architecture/networking
[2]: /ko/observability_pipelines/legacy/architecture/availability_disaster_recovery