---
title: Log Management
kind: Documentation
description: "Configure your Datadog Agent to gather logs from your host, containers & services."
aliases:
    - /guides/logs/
    - /agent/logs/
further_reading:
- link: "logs/log_collection"
  tag: "Documentation"
  text: "Configure your Datadog Agent to gather logs from your host, containers, and services."
- link: "logs/processing"
  tag: "Documentation"
  text: Learn how to process your logs
- link: "logs/explorer/"
  tag: "Documentation"
  text: Search through all of your logs and perform Log Analytics.
---

The Log Management solution is an all-in-one comprehensive solution that comprises [collection][34], [processing][33], live tailing, [exploration][17], [graphing][23], [dashboarding][36], [alerting][35] and archival over all the logs generated by [your application][16], and your infrastructure.

{{< vimeo 243374392 >}}

## Log Collection

Log collection is the beginning of your journey in the wonderful world of log-management. Use the [Datadog Agent][6] to collect logs directly from your hosts or your containerized environments. You can collect AWS service logs with Datadog's [AWS Lambda function](#from-aws-services).If you are already using a log-shipper daemon, refer to the dedicated documentation for [Rsyslog][1], [Syslog-ng][2], [NXlog][3], [FluentD][4], and [Logstash][5].

Integrations and Log Collection are intimately tied together, by collecting Logs the right way you make sure to auto-configure all the the subsequent components such as [processing][33], [parsing][29], and [facets][18] in the Explorer. **[Discover the log integrations supported by Datadog][37]**. You can also define custom log sources if there isn't an integration for your source yet.

<div class="alert alert-warning">
<a href="https://docs.datadoghq.com/integrations/#cat-log-collection">Consult the current list of available supported integrations</a>.
</div>

Find below the different ways and places to collect logs.

### From your hosts

Follow the [Datadog Agent installation instructions][6] to start forwarding logs alongside your metrics and traces.
The Agent can [tail log files][7] or [listen for logs sent over UDP / TCP][8], and you can configure it to [filter out logs][9], [scrub sensitive data][10], or  aggregate [multi line logs][11].

### From a Docker environment

The Datadog Agent can [collect logs directly from container stdout/stderr][14] without using a logging driver. When the Agent's Docker check is enabled, container and orchestrator metadata are automatically added as tags to your logs.

It is possible to collect logs from all your containers or only a subset filtered by container image, label, or name. Autodiscovery can also be used to configure log collection directly in the container labels.

In Kubernetes environments you can also leverage [the daemonset installation][15].

### From AWS services

The Datadog Agent can be used to collect logs directly from ECS or EC2 instances and applications running on them.

However, AWS services logs are collected thanks to Datadog's [Lambda function][12]. Triggers are then defined ([manually or automatically][13]) to forward logs from any S3 bucket, Cloudwatch Log group, or Cloudwatch events.

### From a custom forwarder

Any custom process or [logging library][16] able to forward logs through **TCP** can be used in conjuntion with Datadog Logs. The secure TCP endpoint is `intake.logs.datadoghq.com:10516` (or port `10514` for insecure connections). 

You must prefix the log entry with your [Datadog API Key][38], e.g.:

```
<DATADOG_API_KEY> this is my log
```

Test it manually with telnet:

```
telnet intake.logs.datadoghq.com 10514 
<DATADOG_API_KEY> Log sent directly via TCP
```

This will produce the following result in your [live tail page][39]: 

{{< img src="logs/custom_log_telnet.png" alt="Custom telnet" responsive="true" style="width:70%;">}}

### Datadog Logs Endpoints

Find below all the endpoints that can be used to send logs to Datadog:

{{< tabs >}}
{{% tab "US Region" %}}

| Endpoint                           | Port    | Description                                                                                                           |
| :--------------------------------- | :------ | :-------                                                                                                              |
| `agent-intake.logs.datadoghq.com`  | `10516` | Endpoint used by the agent to send logs in protobuf format over a SSL encrypted TCP connection.                       |
| `intake.logs.datadoghq.com`        | `10516` | Endpoint used by custom forwarders to send logs in raw, Syslog, or JSON format over a SSL encrypted TCP connection.   |
| `intake.logs.datadoghq.com`        | `10514` | Endpoint used by custom forwarders to send logs in raw, Syslog, or JSON format format over a TCP connection.          |
| `lambda-intake.logs.datadoghq.com` | `10516` | Endpoint used by the Lambda function to send logs in raw, Syslog, or JSON format over a SSL encrypted TCP connection. |


{{% /tab %}}
{{% tab "EU Region" %}}

| Endpoint                           | Port    | Description                                                                                                           |
| :--------------------------------- | :------ | :-------                                                                                                              |
| `agent-intake.logs.datadoghq.eu`   | `443`   | Endpoint used by the agent to send logs in protobuf format over SSL encrypted TCP connection.                         |
| `tcp-intake.logs.datadoghq.eu`     | `443`   | Endpoint used by custom forwarders to send logs in raw, Syslog, or JSON format over a SSL encrypted TCP connection.   |
| `tcp-intake.logs.datadoghq.eu`     | `1883`  | Endpoint used by custom forwarders to send logs in raw, Syslog, or JSON format format over a TCP connection.          |
| `lambda-intake.logs.datadoghq.eu`  | `443`   | Endpoint used by the Lambda function to send logs in raw, Syslog, or JSON format over a SSL encrypted TCP connection. |


{{% /tab %}}
{{< /tabs >}}


### Reserved attributes

Here are some key attributes you should pay attention to when setting up your project:

| Attribute   | Description                                                                                                                                                                                           |
| :-------    | :------                                                                                                                                                                                               |
| `host`    | The name of the originating host as defined in metrics. We automatically retrieve corresponding host tags from the matching host in Datadog and apply them to your logs. The Agent sets this value automatically.                          |
| `source`  | This corresponds to the integration name: the technology from which the log originated. When it matches an integration name, Datadog automatically installs the corresponding parsers and facets. For example: nginx, postgresql, etc.|
| `service` | The name of the application or service generating the log events. It is used to switch from Logs to APM, so make sure you define the same value when you use both products.                       |
| `message` | By default, Datadog ingests the value of the `message` attribute as the body of the log entry. That value is then highlighted and displayed in the Logstream, where it is indexed for full text search.               |

Your logs are collected and centralized into the [Log Explorer][17] view. You can also search, enrich, and alert on your logs.

{{< img src="logs/log_explorer_view.png" alt="Log Explorer view" responsive="true" >}}

## Further Reading

{{< partial name="whats-next/whats-next.html" >}}

[1]: /integrations/rsyslog
[2]: /integrations/syslog_ng
[3]: /integrations/nxlog
[4]: /integrations/fluentd/#log-collection
[5]: /integrations/logstash/#log-collection
[6]: /logs/log_collection/#getting-started-with-the-agent
[7]: /logs/log_collection/#tail-existing-files
[8]: /logs/log_collection/#stream-logs-through-tcp-udp
[9]: /logs/log_collection/#filter-logs
[10]: /logs/log_collection/#scrub-sensitive-data-in-your-logs
[11]: /logs/log_collection/#multi-line-aggregation
[12]: /integrations/amazon_web_services/#log-collection
[13]: /integrations/amazon_web_services/#enable-logging-for-your-aws-service
[14]: /logs/log_collection/docker/
[15]: /agent/basic_agent_usage/kubernetes/#log-collection-setup
[16]: /logs/log_collection/
[17]: /logs/explore
[18]: /logs/explorer/?tab=facets#setup
[19]: /logs/explorer/search/#create-a-facet
[20]: /logs/explorer/search/#search-syntax
[21]: /logs/explorer/?tab=measures#setup
[22]: /logs/explorer/analytics/#related-logs
[23]: /logs/explorer/analytics/
[24]: /logs/log_collection/#reserved-attributes
[25]: /logs/log_collection/#edit-reserved-attributes
[26]: /logs/processing/#processing-pipelines
[27]: /logs/processing/#attribute-remapper
[28]: /logs/processing/#grok-parser
[29]: /logs/processing/parsing/
[30]: /logs/faq/log-parsing-best-practice/
[31]: /logs/faq/how-to-investigate-a-log-parsing-issue/
[32]: /logs/processing/#processors
[33]: /logs/processing
[34]: /logs/log_collection/
[35]: /monitors/monitor_types/log
[36]: /graphing/dashboards/widgets/#timeseries
[37]: /integrations/#cat-log-collection
[38]: https://app.datadoghq.com/account/settings#api
[39]: https://app.datadoghq.com/logs/livetail
