---
title: Configurations
kind: Documentation
aliases:
  - /integrations/observability_pipelines/vector_configurations/
  - /observability_pipelines/vector_configurations/
  - /observability_pipelines/reference/
further_reading:
  - link: /observability_pipelines/working_with_data/
    tag: Documentation
    text: Working with data using Observability Pipelines
  - link: /observability_pipelines/setup
    tag: Documentation
    text: Set up Observability Pipelines
---

{{< site-region region="gov" >}}
<div class="alert alert-warning">Observability Pipelines is not available on the US1-FED Datadog site.</div>
{{< /site-region >}}

## Overview

Observability Pipelines Worker configurations can collect, transform, and route your data[*](#support) from any source to any destination. The configuration file supports YAML, TOML, and JSON. The three main configuration components are sources, transforms, and sinks.

## Set up an example source

[Source components][1] define how the Observability Pipelines Worker collects or receives data from observability data sources.

Create a YAML configuration file and add the following source example:

{{< tabs >}}
{{% tab "YAML" %}}

```yaml
sources:
  generate_syslog:
    type: demo_logs
    format: syslog
    count: 100
```

{{% /tab %}}
{{% tab "TOML" %}}

```toml
[sources.generate_syslog]
   type = "demo_logs"
   format = "syslog"
   count = 100
```

{{% /tab %}}
{{% tab "JSON" %}}

```json
"sources": {
    "generate_syslog": {
      "type": "demo_logs",
      "format": "syslog",
      "count": 100
    }
}
```

{{% /tab %}}
{{< /tabs >}}

This `source` component has a unique ID of `generate_syslog`. This unique ID is important for transforming and routing the data with the`sink` component.

`type` is the source type from which the Observability Pipelines Worker collects observability data. This example uses a `demo_logs` source, which creates sample log data that enables you to simulate different types of events in various formats. The `format` option tells the `demo_logs` source which type of logs to emit, in this case, Syslog format. The `count` option tells the `demo_logs` source how many lines to emit.

See all supported sources in the [Sources documentation][1].

## Set up an example transform

Use the following example to define a [transform component][2] that manipulates the data collected from the `demo_logs` source.

{{< tabs >}}
{{% tab "YAML" %}}

```yaml
transforms:
  remap_syslog:
    inputs:
      - generate_syslog
    type: remap
    source: |2
        structured = parse_syslog!(.message)
        . = merge(., structured)
```

{{% /tab %}}
{{% tab "TOML" %}}

```toml
[transforms.remap_syslog]
   inputs = ["generate_syslog" ]
   type = "remap"
   source = '''
     structured = parse_syslog!(.message)
     . = merge(., structured)
'''
```

{{% /tab %}}
{{% tab "JSON" %}}

```json
"transforms": {
    "remap_syslog": {
      "inputs": [
        "generate_syslog"
      ],
      "type": "remap",
      "source": "  structured = parse_syslog!(.message)\n  . = merge(., structured)\n"
    }
  }
```

{{% /tab %}}
{{< /tabs >}}

In this `transforms.remap_syslog` component, the `inputs` option is set to `generate_syslog`, which means it receives events from the previously defined `generate_syslog` source. The transform's component type is `remap`.

The `source` contains the list of remapping transformations to apply to each event that the Observability Pipelines Worker receives. In this example, only one operation, `parse_syslog`, is performed, but multiple operations can be added.

The  `parse_syslog` function receives a single field called `message`, which contains the Syslog event that is generated in the `generate_syslog` source. This function parses the content of the Syslog-formatted message and emits it as a structured event.

This transform example showcases only a portion of the Observability Pipelines Worker's ability to shape and transform your data[*](#support). See the [Transforms documentation][2] for all supported transforms, ranging from sampling, filtering, enrichment, and more.

## Set up an example sink

With the data parsed in the `transform` component, use the following [sink][3] example to route the data to a destination.

{{< tabs >}}
{{% tab "YAML" %}}

```yaml
sinks:
  emit_syslog:
    inputs:
      - remap_syslog
    type: console
    encoding:
      codec: json
```

{{% /tab %}}
{{% tab "TOML" %}}

```toml
[sinks.emit_syslog]
inputs = [ "remap_syslog" ]
type = "console"

  [sinks.emit_syslog.encoding]
  codec = "json"
```

{{% /tab %}}
{{% tab "JSON" %}}

```json
"sinks": {
    "emit_syslog": {
      "inputs": [
        "remap_syslog"
      ],
      "type": "console",
      "encoding": {
        "codec": "json"
      }
    }
}
```

{{% /tab %}}
{{< /tabs >}}

This `sink` (or destination) component has the ID of `emit_syslog`. The `inputs` option specifies that the events generated by the `remap_syslog` transform are processed with this sink. The `encoding` option tells the sink to emit the events in JSON format.

See the [Sinks documentation][3] for all supported sinks.

## Put it all together

With these three basic components, a source, transform, and sink, you now have a working Observability Pipelines configuration file.

{{< tabs >}}
{{% tab "YAML" %}}

```yaml
sources:
  generate_syslog:
    type: demo_logs
    format: syslog
    count: 100
transforms:
  remap_syslog:
    inputs:
      - generate_syslog
    type: remap
    source: |2
        structured = parse_syslog!(.message)
        . = merge(., structured)

sinks:
  emit_syslog:
    inputs:
      - remap_syslog
    type: console
    encoding:
      codec: json
```

{{% /tab %}}
{{% tab "TOML" %}}

```toml
[sources.generate_syslog]
type = "demo_logs"
format = "syslog"
count = 100

[transforms.remap_syslog]
inputs = [ "generate_syslog" ]
type = "remap"
source = '''
  structured = parse_syslog!(.message)
  . = merge(., structured)
'''

[sinks.emit_syslog]
inputs = [ "remap_syslog" ]
type = "console"

  [sinks.emit_syslog.encoding]
  codec = "json"
```

{{% /tab %}}
{{% tab "JSON" %}}

```json
{
  "sources": {
    "generate_syslog": {
      "type": "demo_logs",
      "format": "syslog",
      "count": 100
    }
  },
  "transforms": {
    "remap_syslog": {
      "inputs": [
        "generate_syslog"
      ],
      "type": "remap",
      "source": "  structured = parse_syslog!(.message)\n  . = merge(., structured)\n"
    }
  },
  "sinks": {
    "emit_syslog": {
      "inputs": [
        "remap_syslog"
      ],
      "type": "console",
      "encoding": {
        "codec": "json"
      }
    }
  }
}
```

{{% /tab %}}
{{< /tabs >}}

Run the following command to compile and run this configuration:

```
vector --config ./<configuration_filename>
```

If successfully setup, the parsed demo logs are printed in JSON format.

## Further Reading

{{< partial name="whats-next/whats-next.html" >}}

[1]: /observability_pipelines/reference/sources/
[2]: /observability_pipelines/reference/transforms/
[3]: /observability_pipelines/reference/sinks/

---

<a name="support"></a> * Observability Pipelines support logs. Support for metrics is in beta.