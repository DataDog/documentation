---
title: Enrichment Table Processor
disable_toc: false
products:
- name: Logs
  icon: logs
  url: /observability_pipelines/configuration/?tab=logs#pipeline-types
---

{{< product-availability >}}

## Overview

Logs can contain information like IP addresses, user IDs, or service names that often needs additional context. The Enrichment Table processor allows you to use lookup datasets, stored in Datadog [Reference Tables][2], local files, or MaxMind GeoIP tables, to add context to your logs. The processor matches logs based on a specified key and appends information from your lookup file to the event. Using Reference Tables, you can connect to and enrich logs with SaaS-based datasets directly stored in ServiceNow, Snowflake, S3, and more.

## Use cases for enriching logs from integrations

| Vendor            | What is it? | Enrichment use cases |
| ----------------- | ----------- | -------------------- |
| ServiceNow (CMDB) | An IT service management platform with a Configuration Management Database (CMDB) that tracks infrastructure assets, applications, and dependencies. | - Enrich logs with infra ownership and dependency context (for example, which team owns the host and the business unit the team supports).<br>- Add information directly from CMDB records to telemetry. |
| Snowflake         | A cloud-native data warehouse/lake that centralizes structured and semi-structured data. | - Adding customer metadata (account tier, region, SLA) to logs. <br>- Joining security events with user or asset attributes stored in Snowflake. |
| Databricks        | A cloud-based data lakehouse used for machine learning (ML), advanced analytics, and big data workloads. | - Adding predictions or scores generated by ML models, such as fraud likelihoods, anomaly detection results.<br>- Reference datasets stored in Databricks such as customer profiles, device information, or security info |
| Salesforce        | A customer Relationship Management (CRM) tool used to track and store sales opportunities, accounts, contacts, deals, and contracts. | - Attaching customer and account information like industry, ARR, owner, to operational logs for prioritizing incidents.<br>- Enrich marketing or sales-focused dashboards with operational signals like latency spikes tied to customers. |
| Cloud object storage (Amazon S3, Azure Blob Storage, Google Cloud Storage) | Scalable object storage services used to store large volumes of structured and unstructured reference data. | Enriching logs with externally maintained reference datasets, such as threat intelligence feeds, allow and deny lists, asset inventories, compliance mappings stored as CSVs, or other files and updated regularly |

## Setup

To set up the enrichment table processor:

1. Click **Add enrichment**.
1. Define a **filter query**. Only logs that match the specified [filter query](#filter-query-syntax) are processed. All logs, regardless of whether they match the filter query, are sent to the next step in the pipeline.
1. In the **Set lookup mapping** section, select the type of lookup dataset you want to use.
  {{< tabs >}}
  {{% tab "Reference Table" %}}

  1. Select the Reference Table in the dropdown menu.
      - [Reference Tables][1] allow you to store information like customer details, asset lists, and service dependency information in Datadog.
      - See [Using reference tables](#using-reference-tables) for more information.
  1. Click **Manage** to be directed to the Reference Tables configuration page.
  1. (Optional) Select specific columns with which to enrich your logs.
      - Observability Pipelines enriches logs with all columns in the table by default. If you want to cherrypick columns, you can specify certain attributes to be added. Only selected attributes are enriched.
  1. Enter a Datadog Application key identifier. [Application keys][2] are used by Observability Pipelines to access Datadog's programmatic API when enriching data.
      - Configure your application key in your organization settings under the [Service Accounts][5] page before you deploy the pipeline.
      - Associate your application keys with a [Service Account][3] (not a personal Datadog user account).
      - Application keys can be viewed exactly once, when you create them. Copy and store the value in your preferred secrets manager. If you lose or forget a service account key, revoke it and create another one.
      - Limit your application key to the ['reference_tables_read'][4] scope.
  1. Enter the source attribute of the log. The source attribute's value is what you want to find in the reference table.
  1. Enter the target attribute. The target attribute's value stores, as a JSON object, the information found in the reference table.
  1. Click **Save**.

  [1]: https://docs.datadoghq.com/reference_tables/?tab=cloudstorage#reference-table-limits
  [2]: https://docs.datadoghq.com/account_management/api-app-keys/#application-keys
  [3]: https://docs.datadoghq.com/account_management/org_settings/service_accounts#service-account-application-keys
  [4]: https://docs.datadoghq.com/account_management/rbac/permissions/#reference-tables
  [5]: https://app.datadoghq.com/organization-settings/service-accounts

  {{% /tab %}}
  {{% tab "File" %}}

  1. Enter the file path.
    - **Note**: All file paths are made relative to the configuration data directory, which is `/var/lib/observability-pipelines-worker/config/` by default. The file must be owned by the `observability-pipelines-worker group` and `observability-pipelines-worker` user, or at least readable by the group or user. See [Advanced Worker Configurations][1] for more information.
  1. Enter the column name. The column name in the enrichment table is used for matching the source attribute value. See the [Enrichment file example](#enrichment-file-example).
  1. Enter the source attribute of the log. The source attribute's value is what you want to find in the reference table.
  1. Enter the target attribute. The target attribute's value stores, as a JSON object, the information found in the reference table.
  1. Click **Save**.

  [1]: /observability_pipelines/configuration/install_the_worker/advanced_worker_configurations/

  {{% /tab %}}
  {{% tab "GeoIP" %}}

  1. For GeoIP, enter the GeoIP path to your `.mmdb` file relative to the `<DD_OP_DATA_DIR>/config` directory.
    - **Note**: All file paths are made relative to the configuration data directory, which is `/var/lib/observability-pipelines-worker/config/` by default. The file must be owned by the `observability-pipelines-worker group` and `observability-pipelines-worker` user, or at least readable by the group or user. See [Advanced Worker Configurations][1] for more information.
  1. Enter the source attribute of the log. The source attribute's value is what you want to find in the reference table.
  1. Enter the target attribute. The target attribute's value stores, as a JSON object, the information found in the reference table.
  1. Click **Save**.

  [1]: /observability_pipelines/configuration/install_the_worker/advanced_worker_configurations/

  {{% /tab %}}
  {{< /tabs >}}

##### Enrichment file example

For this example, `merchant_id` is used as the source attribute and `merchant_info` as the target attribute.

This is the example reference table that the enrichment processor uses:

| merch_id | merchant_name   | city      | state    |
| -------- | --------------- | --------- | -------- |
| 803      | Andy's Ottomans | Boise     | Idaho    |
| 536      | Cindy's Couches | Boulder   | Colorado |
| 235      | Debra's Benches | Las Vegas | Nevada   |

`merch_id` is set as the column name the processor uses to find the source attribute's value. **Note**: The source attribute's value does not have to match the column name.

If the enrichment processor receives a log with `"merchant_id":"536"`:

- The processor looks for the value `536` in the reference table's `merch_id` column.
- After it finds the value, it adds the entire row of information from the reference table to the `merchant_info` attribute as a JSON object:

```
merchant_info {
    "merchant_name":"Cindy's Couches",
    "city":"Boulder",
    "state":"Colorado"
}
```

## How the processor works

### Using Reference Tables

The Reference Tables processor pulls rows on demand and caches them locally.

When the processor encounters a log that does not have a corresponding row in the cache, the log data is buffered in memory until the row is retrieved from the reference table. The buffer has a maximum capacity of 100,000 logs. If that limit is reached, the buffer begins sending the oldest logs downstream without enrichment. The processor does not exert upstream backpressure.

A circuit breaker opens if an authentication error occurs while connecting to the reference table or after a series of failed requests. This automatically flushes buffered logs downstream without enrichment, to prevent the logs from waiting indefinitely and cause the buffer to stop accepting new logs. The processor periodically retries requests and automatically closes the circuit to resume normal operations when a request succeeds.

Errors that cause a log to be sent without enrichment can seen in the Worker logs and increments the [`pipelines.component_errors_total` metric](#processor-metrics).

Table rows are persisted in the cache for about 10 minutes. After that, they are evicted or refreshed.

Datadog does not recommend using the processor on a log field with high cardinality (more than 5,000 possible values). The Reference Tables API is subject to rate limits and might deny Worke requests. Reach out to [Datadog support][3] if you continue to notice rate limit warnings in the Worker logs while running the processor.

### Metrics

#### Processor metrics

The following are custom processor metrics tagged with `component_type=enrichment_table` and `component_id=<processor_id>`.

`pipelines.enrichment_rows_not_found_total`
: This counter is incremented for each processed log that does not have a corresponding row in the table.

`pipelines.component_errors_total`
: This common metric is incremented by the processor for each log that cannot be enriched because of an error. These errors are reported with the tag `error_code=did_not_enrich_event`.
: The tag `reason` may contain the following values:<br>- `target_exists`: The target value to store the enriched data already exists and was not an object.<br>- `too_many_pending_lookups`: The buffer or lookup queue is full.<br>- `lookup_failed`: The lookup key was not found in the log, was not a string, or the circuit breaker was opened.

#### Buffer metrics

Common buffer metrics (when enabled) are also reported for the processor buffer:

- `pipelines.buffer_events`
- `pipelines.buffer_bytes`
- `pipelines.buffer_received_events_total`
- `pipelines.buffer_received_bytes_total`
- `pipelines.buffer_sent_events_total`
- `pipelines.buffer_sent_bytes_total`

{{% observability_pipelines/metrics/buffer %}}

Those metrics are tagged with `component_type=enrichment_table`, `component_id=<processor_id>`, and `buffer_id=enrichment_table_buffer`.

#### Reference table metrics

These metrics are produced by an internal controller shared by all processors consuming the same Reference Table. Therefore, they are reported with the following tags:

- `component_type:enrichment_table`
- `component_id:reference_table_<table-id>`

`pipelines.enrichment_rows_not_found_total`
: This counter is incremented for each processed log that does not have a corresponding row in the table.

`pipelines.reference_table_cached_rows`
: This gauge metric reports the number of rows stored in the local cache. The tag `found:true` reports rows existing in the table, and `found:false` reports rows that do not exist in the table. The cache has a maximum capacity of 100,000 entries.

`pipelines.reference_table_queued_keys`
: This gauge metric reports the number of row keys waiting to be read from the Reference Tables API. The queue has a maximum capacity of 5,000 keys. When a log attempts to insert a key that would exceed this limit, the log is immediately sent downstream without enrichment.

`pipelines.reference_table_fetched_keys_total`
: For each request sent to the Reference Tables API, this counter is incremented with the number of rows fetched in that request.

**Note**: Common HTTP client metrics are also reported:

- `pipelines.http_client_requests_sent_total`
- `pipelines.http_client_responses_total`
- `pipelines.http_client_errors_total`
- `pipelines.http_client_rtt_seconds`

{{% observability_pipelines/processors/filter_syntax %}}

[1]: /observability_pipelines/configuration/install_the_worker/advanced_worker_configurations/
[2]: /reference_tables/?tab=cloudstorage
[3]: /help/