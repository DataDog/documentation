---
title: NeMo Evaluations
---
{{< site-region region="gov" >}}
<div class="alert alert-warning">LLM Observability is not available in the selected site ({{< region-param key="dd_site_name" >}}).</div>
{{< /site-region >}}

## Overview
NVIDIA NeMo Evaluator is a cloud-native microservice that streamlines the evaluation of large language models (LLMs) by automating benchmarking against academic standards, custom datasets, and metrics like Accuracy, ROUGE, BLEU, and LLM-as-a-judge scoring. It provides structured feedback on model performance, enabling users to assess foundation and custom models efficiently and continuously optimize them for specific applications


## Using NVIDIA NeMo and LLM Observability

Evaluations from NVIDIA NeMo Evaulator can be uploaded to Datadog LLM Observability using the [LLM Observability Python SDK][1].

1. NeMo evaluation data need to have the `span` and `trace` IDs as metadata on the output data for the evaluation service, which can be captured on the `source` field. In addition, there should be an ID for joining the output data from the LLM with the evaluation data rows generated by the NeMo evaluation service. The `span` and `trace` IDs can be obtained from the LLM Observability SDK. Use `LLMObs.export_span()` to export the current span while tracing the workflow that includes the LLM call.

```python
from ddtrace.llmobs import LLMObs

LLMObs.enable(ml_app="nemos-demo")

import os
from openai import OpenAI

oai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

with LLMObs.workflow(name="handle_user_input"):
  # RAG steps, other input parsing, data validation.
  response = oai_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": "You are a helpful cooking assistant. Please reject any non-food related questions."},
      {"role": "user", "content": "What are the ingredients in cake?"},
    ],
  )

  span_context = LLMObs.export_span()
```

The following span from asking this chat bot `What are the ingredients in cake?` can be viewed in the LLM Observability traces view:

{{< img src="llm_observability/nemo-demo-before-eval.png" alt="An LLM Observability trace" style="width:100%;" >}}

2. The data saved from the `LLMObs.export_span()` calls should be saved and joined with the input and output data for the NeMo evaluation service. Format the data in the `source` metadata field as a string with each entry separated by a semicolon, and the key-value pairs separated by an equals sign:

```json
[
  {
    "input": {
      "prompt": "What are the ingredients in cake?",
      "ideal_response": "Cake is made of flour, sugar, eggs, and milk",
      "category": "food",
      "source": "trace_id=0;span_id=1;question_id=1"
    },
    "response": "The ingredients in a basic cake recipe typically include flour, sugar, eggs, butter or oil, leavening agents like baking powder or baking soda, and a liquid such as milk or water. Additional flavorings such as vanilla extract or cocoa powder can also be added for variety.",
    "llm_name": "gpt-3.5-turbo",
  }
]
```

3. Download the results from the NeMo evaluation service, and save it as a JSON (or `jsonl` for larger evaluations runs):

```json
{"question_id": 1, "model": "meta/llama-3.1-8b-instruct", "judge": ["meta/llama-3.1-8b-instruct", "single-v1"], "user_prompt": "[Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n\n[Question]\nWhat are the ingredients in cake?\n\n[The Start of Assistant's Answer]\nThe ingredients in a basic cake recipe typically include flour, sugar, eggs, butter or oil, leavening agents like baking powder or baking soda, and a liquid such as milk or water. Additional flavorings such as vanilla extract or cocoa powder can also be added for variety.\n[The End of Assistant's Answer]", "judgment": "Rating: [[8]] The answer clearly denotes the ingredients needed to make a cake", "score": 8, "turn": 1, "tstamp": 1740429821.1071315}
```

4. Submit NeMo Evaluator's model evaluation scores as custom evaluations by joining the generated result with the LLM Observability trace data from the output of the LLM call(s).

```python
import json

from ddtrace.llmobs import LLMObs
LLMObs.enable(
    # Enable the LLM Observability SDK with the same ml_app name as original application
    ml_app="nemos-demo",
)

# modify the following paths to the actual files as needed
OUTPUTS_FILE = 'outputs.json'
SCORES_FILE = 'scores.jsonl'

JOIN_KEY = 'question_id'

def parse_json(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    return data

def parse_jsonl(file_path):
    with open(file_path, 'r') as f:
        data = [json.loads(line) for line in f]
    return data

outputs = parse_json(OUTPUTS_FILE)
scores = parse_jsonl(SCORES_FILE)

def parse_source_into_dict(source: str) -> dict:
    meta_dict = {}
    for meta in source.split(';'):
        key, value = meta.split('=')
        meta_dict[key] = value
    return meta_dict

def find_score(join_key_value: str) -> dict:
    for score in scores:
        if str(score[JOIN_KEY]) == join_key_value:
            return score
    return None


for output in outputs:
    source = output['input']['source']
    meta = parse_source_into_dict(source)

    join_key_value = meta[JOIN_KEY]
    score_row = find_score(join_key_value)
    if score_row is None:
        print(f"ID {join_key_value} not found in scores")
        continue

    LLMObs.submit_evaluation(
      span_context={
        "trace_id": meta['trace_id'],
        "span_id": meta['span_id']
      },
      metric_type="score", # Custom evaluation metric type - change as needed, either "score" or "categorical"
      label="quality_assessment", # Custom evaluation label - change as needed
      value=score_row['score'],
      metadata={
        # add additional metadata as needed
        "model": score_row['model'],
        "judgement": score_row['judgment']
      }
    )
```

The NeMo Evaluator's model evaluation scores can now be viewed attached to the LLM Observability trace in Datadog:

{{< img src="llm_observability/nemo-demo-after-eval.png" alt="An LLM Observability trace with a custom evaluation attached from the NeMo evaluation results" style="width:100%;" >}}

You can view a breakdown of your NeMo Evaluator's model evaluation results in LLM Observability's dashboard and Application Overview, as well as overlay the evaluation results on topic clusters generated on the [Cluster Map][2]. By clicking on `Options` above the traces list, you can add the evaluation scores as a column to the traces table to view the results at a glance on the traces list:

{{< img src="llm_observability/nemo-demo-traces-view-eval.png" alt="An LLM Observability traces list with a custom evaluation attached from the NeMo evaluation results" style="width:100%;" >}}

[1]: /llm_observability/setup/sdk/python
[2]: /llm_observability/cluster_map