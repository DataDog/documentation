stages:
  - bootstrap
  - build
  - post-deploy
  - cleanup

variables:
  PREVIEW_CONFIG: "config/preview.yaml"
  LIVE_CONFIG: "config/live.yaml"
  HUGO_CACHEDIR: "${CI_PROJECT_DIR}/hugo_cache/"
  ARTIFACT_RESOURCE: "public"
  LIVE_DOMAIN: "https://docs.datadoghq.com/"
  PREVIEW_DOMAIN: "https://docs-staging.datadoghq.com/"
  KUBERNETES_SERVICE_ACCOUNT_OVERWRITE: documentation
  KUBERNETES_CPU_REQUEST: 8
  KUBERNETES_MEMORY_REQUEST: 32Gi
  KUBERNETES_MEMORY_LIMIT: 32Gi
  # datadog-ci
  DATADOG_SITE: 'datadoghq.com'
  DATADOG_SUBDOMAIN: 'dd-corpsite'
  FORCE_COLOR: '1'
  GIT_DEPTH: 50
  DEFAULT_TAGS: "environment:${CI_ENVIRONMENT_NAME},job_name:${CI_JOB_NAME},job_stage:${CI_JOB_STAGE},project_name:${CI_PROJECT_NAME},docker_image:${CI_REGISTRY_IMAGE},pipeline_id:${CI_PIPELINE_ID},service:gitlab,pipeline_source:${CI_PIPELINE_SOURCE}"
  FF_USE_FASTZIP: "true"
  # These can be specified per job or per pipeline
  ARTIFACT_COMPRESSION_LEVEL: "fast"
  CACHE_COMPRESSION_LEVEL: "fast"

  # gitlab checkout
  DOCS_REPO_NAME: documentation
  REPO_URL: "https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/${DOCS_REPO_NAME}.git"
  # yarn/node cache dirs
  YARN_CACHE_FOLDER: "${CI_PROJECT_DIR}/.yarn/cache/"
  PREVIEW_PATTERN: '/.+?\/[a-zA-Z0-9_-]+/'
  PREVIEW_ERROR_LOG: "/go/src/github.com/DataDog/documentation/preview_error.log"
  PIP_CACHE_DIR: "${CI_PROJECT_DIR}/.pipcache/"


# ================== copy scripts =============== #

# set policy for the common case
# hugo cache is used only in the build job so lets pull-push
.hugo_cache: &hugo_cache
  - key:
      files:
        - go.sum
    paths:
      - ${HUGO_CACHEDIR}
    policy: pull-push

# set policy for the common case
# yarn cache is used in a lot but only built during build job so lets default to pull and explicit push in build job
.yarn_cache: &yarn_cache
  - key:
      files:
        - yarn.lock
    paths:
      - package.json
      - yarn.lock
      - ${YARN_CACHE_FOLDER}
    policy: pull

.yarn_cache_pull_push: &yarn_cache_pull_push
  - key:
      files:
        - yarn.lock
    paths:
      - package.json
      - yarn.lock
      - ${YARN_CACHE_FOLDER}
    policy: pull-push

.pip_cache: &pip_cache
  - key:
      files:
        - local/etc/requirements3.txt
    paths:
      - ${PIP_CACHE_DIR}
    policy: pull-push

# ==================== rules ==================== #
.preview_rules: &preview_rules
  rules:
    - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH && $CI_COMMIT_BRANCH =~ $PREVIEW_PATTERN
      when: on_success

.preview_manual_rules: &preview_manual_rules
  rules:
    - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH && $CI_COMMIT_BRANCH =~ $PREVIEW_PATTERN
      when: manual

.live_rules: &live_rules
  rules:
    - if: $CI_COMMIT_REF_NAME == "master"
      when: on_success

.live_schedule_rules: &live_schedule_rules
  rules:
    - if: $CI_COMMIT_REF_NAME == "master" && $CI_PIPELINE_SOURCE == "schedule"
      when: on_success

.sparse_before_script: &sparse_before_script |
  [ -d local/bin ] && find local/bin/ -type f -exec cp {} /usr/local/bin \\;  # load scripts
  [ -f /usr/local/bin/helpers.sh ] && source /usr/local/bin/helpers.sh
  export GITHUB_TOKEN=$(get_secret 'github_token')
  export SLACK_URL=$(get_secret 'slack_url')
  export WEBOPS_SLACK_URL=$(get_secret 'webops_slack_url')
  export IA_SUBDOMAIN=$(in-isolation get_ia_subdomain)
  git config --global url."https://${GITHUB_TOKEN}@github.com/DataDog".insteadOf https://github.com/DataDog
  export job_start=$(date +%s)

.sparse_clone: &sparse_clone |
  rm -rf ./sparse-documentation
  git clone --branch $CI_COMMIT_REF_NAME --depth 200 --filter=blob:none --no-checkout https://github.com/DataDog/documentation.git sparse-documentation
  cd sparse-documentation
  git sparse-checkout init --cone && git sparse-checkout set assets config content data i18n layouts local snippets static .yarn .github
  echo '!/static/img/' >> .git/info/sparse-checkout
  git checkout $CI_COMMIT_REF_NAME
  git fetch origin master:refs/remotes/origin/master --no-tags
  git diff origin/master...$CI_COMMIT_REF_NAME --minimal --name-status | { grep "^M\|A\|R\|C" || true; } | rev | cut -f1 - | rev | { grep "static/" || true; } >> .git/info/sparse-checkout
  cat .git/info/sparse-checkout
  git checkout $CI_COMMIT_REF_NAME

# Common script is our version of gitlabs before_script
# We do this ourselves because we are cloning manually with sparse clone
.common_script: &common_script
  - "[ -d local/bin ] && find local/bin/ -type f -exec cp {} /usr/local/bin \\;"  # load scripts
  - "[ -d local/etc ] && find local/etc/ -type f -exec cp {} /etc \\;"  # load configs
  - "[ -f /usr/local/bin/helpers.sh ] && source /usr/local/bin/helpers.sh"  # source helpers so they are available in the container
  - chmod +x /usr/local/bin/*.py # make python scripts executable by the runner
  - "[ -f /etc/profile ] && source /etc/profile" # for golang on path
  - mkdir -p logs
  # set common env variables for this shell
  - export GITHUB_TOKEN=$(get_secret 'github_token')
  - export DATADOG_API_KEY=$(get_secret 'dd-api-key')
  - export DATADOG_APP_KEY=$(get_secret 'dd-app-key')
  - export BRANCH=${CI_COMMIT_REF_NAME}
  - export SLACK_URL=$(get_secret 'slack_url')
  - export WEBOPS_SLACK_URL=$(get_secret 'webops_slack_url')
  - export IA_SUBDOMAIN=$(in-isolation get_ia_subdomain)
  - git config --global url."https://${GITHUB_TOKEN}@github.com/DataDog".insteadOf https://github.com/DataDog
  - type build_dogrc &>/dev/null && build_dogrc || echo "build_dogrc not found." # create .dogrc for metrics, events submission
  - export job_start=$(date +%s) # we need the start of each job recorded for the ci reporting

# ================== templates ================== #
.base_template: &base_template
  image: registry.ddbuild.io/ci/websites/webops-site-build:latest
  before_script:
    - *common_script
  tags:
    - "arch:amd64"
  rules:
    - if: $CI_COMMIT_TAG != null # Variable exists on tag workflows
      when: never
    - if: $CI_COMMIT_BRANCH != null # Variable doesnt exists on merge request pipelines or tag pipelines.
      when: on_success

# ================== preview ================== #
# If the branch has a name of <slack-user>/<feature-name> then ci builds a preview site
build_preview:
  <<: *base_template
  <<: *preview_rules
  stage: build
  cache:
    - *hugo_cache
    - *yarn_cache_pull_push
    - *pip_cache
  environment: "preview"
  before_script:
    - *sparse_before_script
  variables:
    URL: ${PREVIEW_DOMAIN}
    CONFIG: ${PREVIEW_CONFIG}
    MESSAGE: ":gift_heart: Your preview site is available! Now running tests..."
    CONFIGURATION_FILE: "./local/bin/py/build/configurations/pull_config_preview.yaml"
    LOCAL: "False"
    GIT_STRATEGY: none
  script:
    - *sparse_clone
    - *common_script
    - dog --config "$HOME/.dogrc" event post "documentation build ${CI_COMMIT_REF_NAME} started" "${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}" --alert_type "info" --tags="${DEFAULT_TAGS}"
    - touch Makefile.config
    - touch preview_error.log # Create log file for errors
    - exec 2> $PREVIEW_ERROR_LOG # We want to output any detected error to a log for use in notifications
    - make dependencies
    - make vector_data
    - make config
    - ./node_modules/.bin/jest --testPathPattern=assets/scripts/tests/
    - yarn run prebuild
    - build_site
    # remove service_checks json as we don't need to s3 push that..
    - rm -rf data/service_checks
    - in-isolation deploy_site > logs/deploy.txt
    # remove static images so we can skip from artifact passed in gitlab
    - remove_static_from_repo
    - notify_slack.py  "Your branch ${CI_COMMIT_REF_NAME} is ready for preview. <${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}| checks running>." --status="#31b834" --previewSite="true"
    # copy artifacts back to root to make it easier to pass to subsequent jobs using existing logic
    - cp -r {$ARTIFACT_RESOURCE,integrations_data,.github,.vale.ini,.htmltest.yml,logs,$PREVIEW_ERROR_LOG} $CI_PROJECT_DIR
    - ci_pass_step "${CI_PROJECT_NAME}-${CI_JOB_NAME}" # run at completion of job, if job fails at any point prior this won't run
  artifacts:
    when: always
    paths:
      - ${ARTIFACT_RESOURCE}
      - ./integrations_data
      - .github/
      - .vale.ini
      - .htmltest.yml
      - ./logs
      - ./public/redirects.txt
      - $PREVIEW_ERROR_LOG
      - ./latest-cached.tar.gz
  after_script:
    - |
      if [ -s $PREVIEW_ERROR_LOG && $CI_JOB_STATUS != 'success' ]; then
        echo "Error Detected: $(cat $PREVIEW_ERROR_LOG)";
      fi # Output the error to the console
  interruptible: true

link_checks_preview:
  <<: *base_template
  <<: *preview_rules
  stage: post-deploy
  cache: {}
  environment: "preview"
  variables:
    URL: ${PREVIEW_DOMAIN}
    GIT_STRATEGY: none
  script:
    - dog --config "$HOME/.dogrc" event post "documentation htmltest ${CI_COMMIT_REF_NAME} started" "${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}" --alert_type "info" --tags="${DEFAULT_TAGS}"
    - find ./public -name "*.html" -exec sed -i -e "s#https://docs-staging.datadoghq.com/$CI_COMMIT_REF_NAME/#/#g" {} +
    - find ./public -name "*.html" -exec sed -i -e "s#/api/v1/#/api/latest/#g" {} +
    - find ./public -name "*.html" -exec sed -i -e "s#/api/v2/#/api/latest/#g" {} +
    - htmltest
  after_script:
    - "[ -f /usr/local/bin/helpers.sh ] && source /usr/local/bin/helpers.sh"  # source helpers
    - RESULT="⛈ htmltest result; the following issues were found:\n$(tail -n +2 ./tmp/.htmltest/htmltest.log)\n--------------------" # output all but first line
    - if grep -q "not exist" tmp/.htmltest/htmltest.log; then notify_slack.py "${RESULT}" --status="#31b834"; fi
  artifacts:
    paths:
      - ./tmp/.htmltest
  interruptible: true

missing_tms_preview:
  <<: *base_template
  <<: *preview_rules
  stage: post-deploy
  environment: "preview"
  allow_failure: true
  cache: {}
  dependencies:
    - build_preview
  script:
    - check_missing_tms
  interruptible: true

algolia_sync_preview:manual:
  <<: *base_template
  stage: post-deploy
  environment: "preview"
  allow_failure: true
  interruptible: true
  timeout: 2h
  cache:
    - *hugo_cache
    - *yarn_cache
    - *pip_cache
  dependencies:
    - build_preview
  script:
    # build the site to get algolia.json ondemand
    - touch Makefile.config preview_error.log
    - make dependencies
    - make config
    - yarn run prebuild
    - build_site
    # end build
    - yarn add atomic-algolia@https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/atomic-algolia-v1.0.2.tgz
    - >
      ALGOLIA_APP_ID=$(get_secret 'algolia_preview_application_id')
      ALGOLIA_ADMIN_KEY=$(get_secret 'algolia_preview_api_key')
      CI_PIPELINE_SOURCE=$CI_PIPELINE_SOURCE
      yarn run algolia:sync
  rules:
    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
      when: never
    - if: '$CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH && $CI_COMMIT_TAG =~ "/^$/"'
      when: manual

algolia_sync_live:
  <<: *base_template
  <<: *live_rules
  stage: post-deploy
  environment: "live"
  allow_failure: true
  interruptible: true
  timeout: 2h
  dependencies:
    - build_live
  script:
    - yarn add atomic-algolia@https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/atomic-algolia-v1.0.2.tgz
    - >
      ALGOLIA_APP_ID=$(get_secret 'algolia_docsearch_application_id')
      ALGOLIA_ADMIN_KEY=$(get_secret 'algolia_docsearch_api_key')
      CI_PIPELINE_SOURCE=$CI_PIPELINE_SOURCE
      yarn run algolia:sync

sourcemaps_preview:
  <<: *base_template
  <<: *preview_rules
  stage: post-deploy
  cache:
    - *yarn_cache
  environment: "preview"
  script:
    - yarn install --immutable
    # Rename any js.map files to include SHA if correlating JS file is fingerprinted.  Hugo's asset build system seems to lose the fingerprint value when generating source maps.
    - find ./public -type f -name "*.*.js" -exec bash -c 'x={}; hash=$(basename $x | cut -d '.' -f 2); map=${x/$hash./}.map; mv $map ${map/.js/.$hash.js}' \;
    - DATADOG_API_KEY="$(get_secret 'dd-api-key')" ./node_modules/.bin/datadog-ci sourcemaps upload ./public/static --service docs --minified-path-prefix "https://docs-staging.datadoghq.com/${CI_COMMIT_REF_NAME}/static/" --release-version "${CI_COMMIT_SHORT_SHA}"
  dependencies:
    - build_preview
  allow_failure: true
  interruptible: true

post_build_success_status_to_spec_repo:
  <<: *base_template
  stage: cleanup
  cache: {}
  environment: "preview"
  script:
    - chmod +x ./local/bin/py/submit_github_status_check.py
    - ./local/bin/py/submit_github_status_check.py -i "$(get_secret 'dd-api-spec-repo-app-id')" -k "$(get_secret 'dd-api-spec-repo-app-key')"
  rules:
    - if: $CI_COMMIT_BRANCH =~ /^datadog-api-spec\/generated.*/
      when: on_success
  interruptible: true
  allow_failure: true

post_build_failure_status_to_spec_repo:
  <<: *base_template
  stage: cleanup
  cache: {}
  environment: "preview"
  script:
    - chmod +x ./local/bin/py/submit_github_status_check.py
    - ./local/bin/py/submit_github_status_check.py -s "failure" -i "$(get_secret 'dd-api-spec-repo-app-id')" -k "$(get_secret 'dd-api-spec-repo-app-key')"
  rules:
    - if: $CI_COMMIT_BRANCH =~ /^datadog-api-spec\/generated.*/
      when: on_failure
  interruptible: true
  allow_failure: true

# ================== live ================== #
build_live:
  <<: *base_template
  <<: *live_rules
  stage: build
  cache:
    - *hugo_cache
    - *yarn_cache_pull_push
    - *pip_cache
  environment: "live"
  retry: 1
  variables:
    CONFIG: ${LIVE_CONFIG}
    URL: ${LIVE_DOMAIN}
    UNTRACKED_EXTRAS: "data"
    CONFIGURATION_FILE: "./local/bin/py/build/configurations/pull_config.yaml"
    LOCAL: "False"
  script:
    - dog --config "$HOME/.dogrc" event post "documentation deploy ${CI_COMMIT_REF_NAME} started" "${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}" --alert_type "info" --tags="${DEFAULT_TAGS}"
    - notify_slack.py "<https://github.com/DataDog/documentation/commit/${CI_COMMIT_SHA}|${CI_COMMIT_SHA:0:8}> sent to gitlab for production deployment. <${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}|details>" --status="#FFD700"
    - touch Makefile.config
    - make clean-examples
    - make dependencies
    - make vector_data
    - yarn run prebuild
    - build_site
    - in-isolation deploy_site
    - in-isolation create_artifact
    - in-isolation create_artifact_ignored
    - in-isolation create_artifact_cached
    - remove_static_from_repo
    - ci_pass_step "${CI_PROJECT_NAME}-${CI_JOB_NAME}" # run at completion of job, if job fails at any point prior this won't run
  artifacts:
    when: on_success
    paths:
      - ${ARTIFACT_RESOURCE}
      - ./integrations_data
      - content/en
      - .htmltest.yml
      - ./public/redirects.txt
    expire_in: 1 week

test_live_link_checks:
  <<: *base_template
  <<: *live_rules
  stage: post-deploy
  cache: {}
  environment: "live"
  variables:
    URL: ${LIVE_DOMAIN}
    GIT_STRATEGY: none
  script:
    - dog --config "$HOME/.dogrc" event post "documentation htmltest ${CI_COMMIT_REF_NAME} started" "${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}" --alert_type "info" --tags="${DEFAULT_TAGS}"
    - find ./public -name "*.html" -exec sed -i -e "s#https://docs.datadoghq.com/#/#g" {} +
    - find ./public -name "*.html" -exec sed -i -e "s#/api/v1/#/api/latest/#g" {} +
    - find ./public -name "*.html" -exec sed -i -e "s#/api/v2/#/api/latest/#g" {} +
    - htmltest
  after_script:
    - "[ -f /usr/local/bin/helpers.sh ] && source /usr/local/bin/helpers.sh"  # source helpers
    - RESULT="⛈ htmltest result; the following issues were found:\n$(tail -n +2 ./tmp/.htmltest/htmltest.log)\n--------------------" # output all but first line
    - if grep -q "not exist" tmp/.htmltest/htmltest.log; then notify_slack.py "${RESULT}" --status="#31b834"; fi
  artifacts:
    paths:
      - ./tmp/.htmltest

sourcemaps_live:
  <<: *base_template
  <<: *live_rules
  stage: post-deploy
  cache:
    - *yarn_cache
  environment: "live"
  script:
    - yarn install --immutable
    # Rename any js.map files to include SHA if correlating JS file is fingerprinted. Hugo's asset build system seems to lose the fingerprint value when generating source maps.
    - find ./public -type f -name "*.*.js" -exec bash -c 'x={}; hash=$(basename $x | cut -d '.' -f 2); map=${x/$hash./}.map; mv $map ${map/.js/.$hash.js}' \;
    - DATADOG_API_KEY="$(get_secret 'dd-api-key')" ./node_modules/.bin/datadog-ci sourcemaps upload ./public/static --service docs --minified-path-prefix "https://docs.datadoghq.com/static/" --release-version "${CI_COMMIT_SHORT_SHA}"
  dependencies:
    - build_live
  allow_failure: true

evaluate_missing_metrics:
  <<: *base_template
  stage: post-deploy
  allow_failure: true
  cache: {}
  environment: "live"
  script:
    - chmod +x ./local/bin/py/missing_metrics.py && ./local/bin/py/missing_metrics.py -k $(get_secret 'dd-demo-api-key') -p $(get_secret 'dd-demo-app-key') -a $(get_secret 'dd-api-key') -b $(get_secret 'dd-app-key')
  rules:
    - if: $CI_COMMIT_REF_NAME == "master"
      when: manual

# Uploads git ignored source (EN) files to Transifex as part of the nightly build.
# The translation syncing pipeline is triggered by github webhook, but these files are not in git.
# This allows some dynamically built content (such as integrations) to be localized automatically.
manage_ignored_translations:on-schedule:
  <<: *base_template
  <<: *live_schedule_rules
  stage: post-deploy
  cache: {}
  environment: "live"
  script:
    - echo "Downloading ignored translation source files..."
    - manage_ignored_translation_files

# This checks anchor links,
# its too noisy to run all the time but we want some insight so its running on schedule
link_checks:on-schedule:
  <<: *base_template
  <<: *live_schedule_rules
  stage: post-deploy
  cache: {}
  environment: "live"
  variables:
    URL: ${LIVE_DOMAIN}
    GIT_STRATEGY: none
  dependencies:
    - build_live
  script:
    - dog --config "$HOME/.dogrc" event post "documentation htmltest ${CI_COMMIT_REF_NAME} started" "${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}" --alert_type "info" --tags="${DEFAULT_TAGS}"
    - find ./public -name "*.html" -exec sed -i -e "s#https://docs.datadoghq.com/#/#g" {} +
    - find ./public -name "*.html" -exec sed -i -e "s#/api/v1/#/api/latest/#g" {} +
    - find ./public -name "*.html" -exec sed -i -e "s#/api/v2/#/api/latest/#g" {} +
    - "sed -i'' -e 's/CheckInternalHash: false/CheckInternalHash: true/' .htmltest.yml"
    - htmltest
  artifacts:
    paths:
      - ./tmp/.htmltest
  interruptible: true
  allow_failure: true

# We are relying on the preview version of this job due to the length of time it takes
# test_missing_tms_live:
#   <<: *base_template
#   <<: *live_rules
#   stage: post-deploy
#   cache: {}
#   environment: "live"
#   dependencies:
#     - build_live
#   script:
#     - check_missing_tms

# template for what runs in pa11y to avoid duplication
.pa11y_live_template: &pa11y_live_template
  image: registry.ddbuild.io/ci/websites/pa11y:v33259941-f0dfbc91
  tags: ["arch:amd64"]
  stage: post-deploy
  cache: []
  variables:
    GIT_STRATEGY: none
  script:
    - unset NODE_OPTIONS
    - cp -r ${ARTIFACT_RESOURCE} /app # copy the artifact to the working diretory of this docker container
    - cd /app # go into working directory
    - export CI_PROJECT_DIR="." # override project dir so pa11y script can locate the generated csv and push to s3
    - node ./index.js --env live --site docs --dd_api_key "$(get_secret 'dd_synthetic_api_key_prod')" # run pa11y
  interruptible: true

run_pa11y:schedule:
  <<: *pa11y_live_template
  dependencies:
    - build_live
  rules:
    - if: $CI_COMMIT_BRANCH == "master" && $CI_PIPELINE_SOURCE == "schedule"
      when: always
    - if: $CI_COMMIT_TAG != null || $CI_COMMIT_BRANCH =~ "/pull/\/.*/"
      when: never
  timeout: 2h

run_pa11y:manual:
  <<: *pa11y_live_template
  allow_failure: true # https://gitlab.com/gitlab-org/gitlab/-/issues/39534 Need to allow failure to not get a blocked status
  rules:
    - if: $CI_COMMIT_TAG != null || $CI_COMMIT_BRANCH =~ "/pull/\/.*/"
      when: never
    - if: $CI_COMMIT_TAG == null
      when: manual

tag_successful_live_pipeline:
  <<: *base_template
  <<: *live_rules
  stage: cleanup
  cache: {}
  environment: "live"
  when: on_success
  script:
    - tag_successful_pipeline

post_success_event_to_dd_live:
  <<: *base_template
  <<: *live_rules
  stage: cleanup
  cache: {}
  environment: "live"
  variables:
    GIT_STRATEGY: none
  script:
    - dog --config "$HOME/.dogrc" event post "documentation deploy ${CI_COMMIT_REF_NAME} succeeded" "${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}" --alert_type "info" --tags="${DEFAULT_TAGS}"
    - dog --config "$HOME/.dogrc" metric post "documentation.pipeline.completed" "1" --tags="${DEFAULT_TAGS},step_status:success"

post_failure_event_to_dd_live:
  <<: *base_template
  stage: cleanup
  cache: {}
  environment: "live"
  variables:
    GIT_STRATEGY: none
  script:
    - dog --config "$HOME/.dogrc" event post "documentation deploy ${CI_COMMIT_REF_NAME} failed" "${CI_PROJECT_URL}/pipelines/${CI_PIPELINE_ID}" --alert_type "error" --tags="${DEFAULT_TAGS},step_status:failure"
    - dog --config "$HOME/.dogrc" metric post "documentation.pipeline.completed" "0" --tags="${DEFAULT_TAGS},step_status:failure"
  rules:
    - if: $CI_COMMIT_REF_NAME == "master"
      when: on_failure

set_redirect_metadata_preview:
  <<: *base_template
  <<: *preview_rules
  variables:
    BUCKET: "datadog-docs-preview"
    SET_REDIRECTS_PREVIEW: "true"
  stage: post-deploy
  cache: []
  environment: "preview"
  dependencies:
    - build_preview
  script:
    - in-isolation run_update_redirect_metadata
  interruptible: true
  allow_failure: true

set_redirect_metadata_live:
  <<: *base_template
  <<: *live_rules
  variables:
    BUCKET: "datadog-docs-live-hugo"
  stage: post-deploy
  cache: []
  environment: "live"
  dependencies:
    - build_live
  script:
    - in-isolation run_update_redirect_metadata
  interruptible: true

run_notranslate_tag:schedule:
  <<: *base_template
  stage: post-deploy
  cache: []
  dependencies:
    - build_live
  timeout: 4h
  script:
    - python3 -m venv ./venv && source ./venv/bin/activate && pip install transifex-python
    - export transifex_api_key="$(get_secret 'transifex_api_key')"
    - source ./venv/bin/activate && python3 ./local/bin/py/add_notranslate_tag.py
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH && $CI_PIPELINE_SOURCE == "schedule"
      when: always
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: manual
      allow_failure: true
    - when: never

# run_notranslate_tag:manual:
#   <<: *base_template
#   <<: *live_schedule_rules
#   allow_failure: true
#   stage: post-deploy
#   timeout: 2h
#   script:
#     - export transifex_api_key="$(get_secret 'transifex_api_key')"
#     - python3 ./local/bin/py/add_notranslate_tag.py
#   rules:
#     - if: $CI_COMMIT_TAG != null || $CI_COMMIT_BRANCH =~ "/pull/\/.*/"
#       when: never
#     - if: $CI_COMMIT_TAG == null
#       when: manual

####################################
#                                  #
#      Failure Notifications       #
#                                  #
####################################
failure_notification_preview:
  <<: *base_template
  stage: cleanup
  environment: "preview"
  variables:
    GIT_STRATEGY: none
  script:
    - |
      if [ -s preview_error.log ]; then
        SLACK_MESSAGE=":siren: A gitlab job for your branch has failed with the following error: $(tail -n 10 preview_error.log)";
      else
        SLACK_MESSAGE=":siren: A gitlab job for your branch has failed due to an error, please check your pipeline"; # Safety in case the error log is empty
      fi
    - notify_slack.py "${SLACK_MESSAGE}" --status="#ab0000" --previewSite="false" --link="${CI_PIPELINE_URL}" --buttonContent="Gitlab Pipeline :looking:"
  rules:
    - if: $CI_COMMIT_BRANCH != $CI_DEFAULT_BRANCH && $CI_COMMIT_BRANCH =~ $PREVIEW_PATTERN
      when: on_failure
